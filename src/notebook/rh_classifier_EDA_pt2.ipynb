{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation and Handling\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "\n",
    "# DB Credentials\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "# from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Handling Imbalanced Data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Gradient Boosting Libraries\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Model Lifecycle Management\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Distributed Computing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier as SparkRFClassifier\n",
    "\n",
    "# Model Interpretability\n",
    "import shap\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "\n",
    "# Automated Feature Engineering\n",
    "import featuretools as ft\n",
    "\n",
    "# Add parent directory to sys.path\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Custom Modules\n",
    "from fetch_data_hook import fetch_sql_code, fetch_sql_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>risk_tolerance</th>\n",
       "      <th>investment_experience</th>\n",
       "      <th>liquidity_needs</th>\n",
       "      <th>platform</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>instrument_type_first_traded</th>\n",
       "      <th>first_deposit_amount</th>\n",
       "      <th>time_horizon</th>\n",
       "      <th>user_id</th>\n",
       "      <th>churn_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>Android</td>\n",
       "      <td>33.129417</td>\n",
       "      <td>stock</td>\n",
       "      <td>40.0</td>\n",
       "      <td>med_time_horizon</td>\n",
       "      <td>895044c23edc821881e87da749c01034</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>med_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>Android</td>\n",
       "      <td>16.573517</td>\n",
       "      <td>stock</td>\n",
       "      <td>200.0</td>\n",
       "      <td>short_time_horizon</td>\n",
       "      <td>458b1d95441ced242949deefe8e4b638</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>med_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>iOS</td>\n",
       "      <td>10.008367</td>\n",
       "      <td>stock</td>\n",
       "      <td>25.0</td>\n",
       "      <td>long_time_horizon</td>\n",
       "      <td>c7936f653d293479e034865db9bb932f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>med_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>Android</td>\n",
       "      <td>1.031633</td>\n",
       "      <td>stock</td>\n",
       "      <td>100.0</td>\n",
       "      <td>short_time_horizon</td>\n",
       "      <td>b255d4bd6c9ba194d3a350b3e76c6393</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>Android</td>\n",
       "      <td>8.187250</td>\n",
       "      <td>stock</td>\n",
       "      <td>20.0</td>\n",
       "      <td>long_time_horizon</td>\n",
       "      <td>4a168225e89375b8de605cbc0977ae91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579</th>\n",
       "      <td>high_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>Android</td>\n",
       "      <td>8.339283</td>\n",
       "      <td>stock</td>\n",
       "      <td>300.0</td>\n",
       "      <td>long_time_horizon</td>\n",
       "      <td>03880c726d8a4e5db006afe4119ad974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5580</th>\n",
       "      <td>med_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>somewhat_important_liq_need</td>\n",
       "      <td>iOS</td>\n",
       "      <td>7.241383</td>\n",
       "      <td>stock</td>\n",
       "      <td>100.0</td>\n",
       "      <td>short_time_horizon</td>\n",
       "      <td>ae8315109657f44852b24c6bca4decd6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5581</th>\n",
       "      <td>med_risk_tolerance</td>\n",
       "      <td>no_investment_exp</td>\n",
       "      <td>very_important_liq_need</td>\n",
       "      <td>both</td>\n",
       "      <td>22.967167</td>\n",
       "      <td>stock</td>\n",
       "      <td>50.0</td>\n",
       "      <td>short_time_horizon</td>\n",
       "      <td>f29c174989f9737058fe808fcf264135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5582</th>\n",
       "      <td>med_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>somewhat_important_liq_need</td>\n",
       "      <td>iOS</td>\n",
       "      <td>10.338417</td>\n",
       "      <td>stock</td>\n",
       "      <td>100.0</td>\n",
       "      <td>long_time_horizon</td>\n",
       "      <td>24843497d1de88b2e7233f694436cb3a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5583</th>\n",
       "      <td>high_risk_tolerance</td>\n",
       "      <td>limited_investment_exp</td>\n",
       "      <td>somewhat_important_liq_need</td>\n",
       "      <td>iOS</td>\n",
       "      <td>18.470950</td>\n",
       "      <td>stock</td>\n",
       "      <td>50.0</td>\n",
       "      <td>long_time_horizon</td>\n",
       "      <td>49ee0531ee9dfbce0e7d9afa1c3d86f4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5584 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           risk_tolerance   investment_experience  \\\n",
       "0     high_risk_tolerance  limited_investment_exp   \n",
       "1      med_risk_tolerance  limited_investment_exp   \n",
       "2      med_risk_tolerance  limited_investment_exp   \n",
       "3      med_risk_tolerance  limited_investment_exp   \n",
       "4     high_risk_tolerance  limited_investment_exp   \n",
       "...                   ...                     ...   \n",
       "5579  high_risk_tolerance  limited_investment_exp   \n",
       "5580   med_risk_tolerance  limited_investment_exp   \n",
       "5581   med_risk_tolerance       no_investment_exp   \n",
       "5582   med_risk_tolerance  limited_investment_exp   \n",
       "5583  high_risk_tolerance  limited_investment_exp   \n",
       "\n",
       "                  liquidity_needs platform  time_spent  \\\n",
       "0         very_important_liq_need  Android   33.129417   \n",
       "1         very_important_liq_need  Android   16.573517   \n",
       "2         very_important_liq_need      iOS   10.008367   \n",
       "3         very_important_liq_need  Android    1.031633   \n",
       "4         very_important_liq_need  Android    8.187250   \n",
       "...                           ...      ...         ...   \n",
       "5579      very_important_liq_need  Android    8.339283   \n",
       "5580  somewhat_important_liq_need      iOS    7.241383   \n",
       "5581      very_important_liq_need     both   22.967167   \n",
       "5582  somewhat_important_liq_need      iOS   10.338417   \n",
       "5583  somewhat_important_liq_need      iOS   18.470950   \n",
       "\n",
       "     instrument_type_first_traded  first_deposit_amount        time_horizon  \\\n",
       "0                           stock                  40.0    med_time_horizon   \n",
       "1                           stock                 200.0  short_time_horizon   \n",
       "2                           stock                  25.0   long_time_horizon   \n",
       "3                           stock                 100.0  short_time_horizon   \n",
       "4                           stock                  20.0   long_time_horizon   \n",
       "...                           ...                   ...                 ...   \n",
       "5579                        stock                 300.0   long_time_horizon   \n",
       "5580                        stock                 100.0  short_time_horizon   \n",
       "5581                        stock                  50.0  short_time_horizon   \n",
       "5582                        stock                 100.0   long_time_horizon   \n",
       "5583                        stock                  50.0   long_time_horizon   \n",
       "\n",
       "                               user_id  churn_flag  \n",
       "0     895044c23edc821881e87da749c01034           0  \n",
       "1     458b1d95441ced242949deefe8e4b638           0  \n",
       "2     c7936f653d293479e034865db9bb932f           0  \n",
       "3     b255d4bd6c9ba194d3a350b3e76c6393           0  \n",
       "4     4a168225e89375b8de605cbc0977ae91           0  \n",
       "...                                ...         ...  \n",
       "5579  03880c726d8a4e5db006afe4119ad974           0  \n",
       "5580  ae8315109657f44852b24c6bca4decd6           1  \n",
       "5581  f29c174989f9737058fe808fcf264135           0  \n",
       "5582  24843497d1de88b2e7233f694436cb3a           0  \n",
       "5583  49ee0531ee9dfbce0e7d9afa1c3d86f4           0  \n",
       "\n",
       "[5584 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df = fetch_sql_code('''\n",
    "WITH temp1 AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp) AS rn,\n",
    "        timestamp::date - ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp)::int AS streak_id\n",
    "    FROM\n",
    "        equity_value_data\n",
    "),\n",
    "temp2 AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        MIN(timestamp::date) AS start_streak_date,\n",
    "        MAX(timestamp::date) AS end_streak_date,\n",
    "        COUNT(*) AS duration_of_above10_streak\n",
    "    FROM\n",
    "        temp1\n",
    "    GROUP BY\n",
    "        user_id, streak_id\n",
    "),\n",
    "temp3 AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        LAG(end_streak_date) OVER (PARTITION BY user_id ORDER BY start_streak_date ASC) AS prev_above10_streak_date,\n",
    "        start_streak_date - LAG(end_streak_date) OVER (PARTITION BY user_id ORDER BY start_streak_date ASC) AS duration_between_above10_streaks\n",
    "    FROM\n",
    "        temp2\n",
    ")\n",
    "SELECT distinct user_id\n",
    "FROM temp3\n",
    "WHERE duration_between_above10_streaks >= 28\n",
    "''')\n",
    "churn_df\n",
    "churn_users = set(churn_df['user_id'].tolist())\n",
    "\n",
    "df = fetch_sql_code('''\n",
    "select * from features_data\n",
    "''')\n",
    "\n",
    "df['churn_flag'] = df['user_id'].apply(lambda x: 1 if x in churn_users else 0 )\n",
    "df\n",
    "num_cols = ['time_spent', 'first_deposit_amount']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5584 entries, 0 to 5583\n",
      "Data columns (total 10 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   risk_tolerance                5584 non-null   object \n",
      " 1   investment_experience         5584 non-null   object \n",
      " 2   liquidity_needs               5584 non-null   object \n",
      " 3   platform                      5584 non-null   object \n",
      " 4   time_spent                    5584 non-null   float64\n",
      " 5   instrument_type_first_traded  5584 non-null   object \n",
      " 6   first_deposit_amount          5584 non-null   float64\n",
      " 7   time_horizon                  5584 non-null   object \n",
      " 8   user_id                       5584 non-null   object \n",
      " 9   churn_flag                    5584 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(7)\n",
      "memory usage: 436.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_spent</th>\n",
       "      <th>first_deposit_amount</th>\n",
       "      <th>churn_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5584.000000</td>\n",
       "      <td>5584.000000</td>\n",
       "      <td>5584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34.509706</td>\n",
       "      <td>633.566805</td>\n",
       "      <td>0.049964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>155.080551</td>\n",
       "      <td>2118.323263</td>\n",
       "      <td>0.217890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.848908</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.474708</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33.823829</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8788.329450</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_spent  first_deposit_amount   churn_flag\n",
       "count  5584.000000           5584.000000  5584.000000\n",
       "mean     34.509706            633.566805     0.049964\n",
       "std     155.080551           2118.323263     0.217890\n",
       "min       0.000000              0.000000     0.000000\n",
       "25%       2.848908             50.000000     0.000000\n",
       "50%      13.474708            100.000000     0.000000\n",
       "75%      33.823829            500.000000     0.000000\n",
       "max    8788.329450          50000.000000     1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk_tolerance                  0\n",
       "investment_experience           0\n",
       "liquidity_needs                 0\n",
       "platform                        0\n",
       "time_spent                      0\n",
       "instrument_type_first_traded    0\n",
       "first_deposit_amount            0\n",
       "time_horizon                    0\n",
       "user_id                         0\n",
       "churn_flag                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To further enhance the exploratory data analysis (EDA) from a statistical perspective—especially given that you're analyzing churn with a small percentage (~5%)—we can expand our analysis to include advanced statistical techniques and hypothesis testing. Here's what I would suggest adding as a skilled statistician with a PhD and data science expertise:\n",
    "\n",
    "1. Descriptive Statistics & Distribution Fitting\n",
    "Before diving into modeling, it's crucial to understand the exact nature of the distributions for numerical features. If any feature deviates significantly from a normal distribution, we might consider applying transformations like log, square root, or exponential.\n",
    "Checking for Normality:\n",
    "Use Q-Q plots and Shapiro-Wilk tests to assess whether the numerical features follow a normal distribution.\n",
    "\n",
    "2. Hypothesis Testing\n",
    "T-Tests and Mann-Whitney U Test for comparing means between churn and non-churn groups:\n",
    "For normally distributed data, we can use independent t-tests to compare the means between churn and non-churn users.\n",
    "For non-normally distributed data, the Mann-Whitney U test is more appropriate.\n",
    "\n",
    "3. Bivariate and Multivariate Hypothesis Testing\n",
    "Chi-Square Test: For testing relationships between categorical variables like platform, risk_tolerance, and churn.\n",
    "ANOVA: If we have more than two groups (e.g., risk tolerance levels), we can use ANOVA to check for significant differences between group means.\n",
    "T-TEST: ?\n",
    "\n",
    "4. Correlation and Multicollinearity Analysis\n",
    "We should also examine the multicollinearity of our features, especially if we are considering logistic regression or other linear models.\n",
    "\n",
    "Variance Inflation Factor (VIF) to detect multicollinearity:\n",
    "\n",
    "5. Advanced Distribution Analysis: Fitting data to other distributions (e.g., Poisson, Exponential, etc.)\n",
    "You can fit various distributions to see which one best describes your numerical data, especially when not normally distributed.\n",
    "\n",
    "6. Multivariate Analysis and Interactions\n",
    "Interaction Terms: To capture relationships between variables and churn, you can explore interaction effects.\n",
    "\n",
    "7. Outlier Detection and Handling\n",
    "Using Z-scores or IQR for outlier detection.\n",
    "\n",
    "8. Final Thoughts on Transformations\n",
    "Based on EDA, we may need to transform features for better model performance:\n",
    "\n",
    "Log Transformation: For highly skewed data, log transformation can normalize distributions.\n",
    "Box-Cox Transformation: Helps normalize data and is more flexible than log transformations.\n",
    "The decision to apply transformations should be based on how well the feature distribution aligns with the assumptions of the chosen models (e.g., logistic regression assumes normality).\n",
    "\n",
    "Summary of Additions:\n",
    "Normality Checks: Q-Q plots and Shapiro-Wilk tests for normality.\n",
    "Hypothesis Testing: T-tests, Mann-Whitney U, Chi-Square, and ANOVA to assess differences between churn and non-churn groups.\n",
    "Transformation Decisions: Log, Box-Cox transformations based on distribution analysis.\n",
    "Correlation and Multicollinearity: VIF and correlation heatmaps.\n",
    "Advanced Distribution Fitting: Fit Poisson or other distributions as necessary.\n",
    "Outlier Detection: Using Z-scores or IQR.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Descriptive Statistics & Distribution Fitting | Checking for Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Q-Q plots and Shapiro-Wilk tests to assess whether the numerical features follow a normal distribution.\n",
    "# Q-Q Plot for normality check\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for col in num_cols:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    stats.probplot(df[col], dist=\"norm\", plot=plt)\n",
    "    plt.title(f'Q-Q plot for {col}')\n",
    "    plt.show()\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "for col in num_cols:\n",
    "    stat, p_value = stats.shapiro(df[col])\n",
    "    print(f'Shapiro-Wilk Test for {col}: p-value = {p_value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the p-value is below a certain threshold (e.g., 0.05), the feature is not normally distributed,\n",
    "# and we might consider applying transformations like log or Box-Cox transformations.\n",
    "df['log_first_deposit'] = np.log1p(df['first_deposit_amount'])  # Log transform\n",
    "df['boxcox_first_deposit'], _ = stats.boxcox(df['first_deposit_amount'] + 1)  # Apply Box-Cox transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing | T-Tests and Mann-Whitney U Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Tests and Mann-Whitney U Test for comparing means between churn and non-churn groups:\n",
    "# For normally distributed data, we can use independent t-tests to compare the means between churn and non-churn users.\n",
    "# For non-normally distributed data, the Mann-Whitney U test is more appropriate.\n",
    "\n",
    "# T-Test for normally distributed variables\n",
    "for col in num_cols:\n",
    "    churned = df[df['churn_flag'] == 1][col]\n",
    "    not_churned = df[df['churn_flag'] == 0][col]\n",
    "    t_stat, p_value = stats.ttest_ind(churned, not_churned, equal_var=False)\n",
    "    print(f'T-Test for {col}: p-value = {p_value}')\n",
    "\n",
    "# Mann-Whitney U Test for non-normally distributed variables\n",
    "for col in num_cols:\n",
    "    churned = df[df['churn_flag'] == 1][col]\n",
    "    not_churned = df[df['churn_flag'] == 0][col]\n",
    "    u_stat, p_value = stats.mannwhitneyu(churned, not_churned)\n",
    "    print(f'Mann-Whitney U Test for {col}: p-value = {p_value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate and Multivariate Hypothesis Testing | Chi-Square Test & ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-Square Test: For testing relationships between categorical variables like platform, risk_tolerance, and churn.\n",
    "# Chi-Square test for categorical variables\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "for col in cat_cols:\n",
    "    contingency_table = pd.crosstab(df[col], df['churn_flag'])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    print(f'Chi-Square Test for {col}: p-value = {p}')\n",
    "\n",
    "#ANOVA: If we have more than two groups (e.g., risk tolerance levels), we can use ANOVA to check for significant differences between group means.\n",
    "# ANOVA test for multiple groups\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "\n",
    "model = ols('first_deposit_amount ~ C(risk_tolerance)', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and Multicollinearity Analysis | VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should also examine the multicollinearity of our features, especially if we are considering logistic\n",
    "# regression or other linear models.\n",
    "\n",
    "# Variance Inflation Factor (VIF) to detect multicollinearity:\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculating VIF for numerical features\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = num_cols\n",
    "vif_data['VIF'] = [variance_inflation_factor(df[num_cols].values, i) for i in range(len(num_cols))]\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Distribution Analysis: Fitting data to other distributions (e.g., Poisson, Exponentia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can fit various distributions to see which one best describes your numerical data,\n",
    "# especially when not normally distributed.\n",
    "# Fit a Poisson distribution for count-like data\n",
    "from scipy.stats import poisson\n",
    "\n",
    "poisson_fit = poisson.fit(df['time_spent'])\n",
    "sns.histplot(df['time_spent'], kde=False, stat=\"density\")\n",
    "plt.plot(np.arange(0, max(df['time_spent']), 1), poisson.pmf(np.arange(0, max(df['time_spent']), 1), *poisson_fit))\n",
    "plt.title('Poisson Distribution Fit for Time Spent')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Analysis | Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction Terms: To capture relationships between variables and churn, you can explore interaction effects.\n",
    "# Creating interaction terms\n",
    "df['time_spent_platform_interaction'] = df['time_spent'] * df['platform']\n",
    "\n",
    "# Exploring interaction using pairplot\n",
    "sns.pairplot(df[['time_spent', 'first_deposit_amount', 'platform', 'churn_flag']], hue='churn_flag')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection and Handling | ZSCORE & IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers using Z-scores\n",
    "from scipy import stats\n",
    "\n",
    "z_scores = np.abs(stats.zscore(df[num_cols]))\n",
    "outliers = (z_scores > 3).sum(axis=0)  # Count of outliers per column\n",
    "print(f'Number of outliers per column: {outliers}')\n",
    "\n",
    "\n",
    "# Need to fill in IAR using BoxPLot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Based on EDA, we may need to transform features for better model performance:\n",
    "\n",
    "Log Transformation: For highly skewed data, log transformation can normalize distributions.\n",
    "Box-Cox Transformation: Helps normalize data and is more flexible than log transformations.\n",
    "The decision to apply transformations should be based on how well the feature distribution aligns with the assumptions of the chosen models (e.g., logistic regression assumes normality).\n",
    "\n",
    "Summary of Additions:\n",
    "Normality Checks: Q-Q plots and Shapiro-Wilk tests for normality.\n",
    "Hypothesis Testing: T-tests, Mann-Whitney U, Chi-Square, and ANOVA to assess differences between churn and non-churn groups.\n",
    "Transformation Decisions: Log, Box-Cox transformations based on distribution analysis.\n",
    "Correlation and Multicollinearity: VIF and correlation heatmaps.\n",
    "Advanced Distribution Fitting: Fit Poisson or other distributions as necessary.\n",
    "Outlier Detection: Using Z-scores or IQR.\n",
    "By adding these methods, we ensure a comprehensive statistical and data science-driven EDA, leading to better understanding of the data structure and\n",
    "subsequent decisions for model selection and feature engineering. Would you like to implement any specific sections first, or would you like the full Jupyter notebook with these enhancements?'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
